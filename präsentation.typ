#let title-page(title: [], subtitle: [], faculty, body) = {
  // set page( numbering: "1")
  set page(
    margin: (rest: 1.5in, left: 2.7in),
    paper: "presentation-16-9",
    fill: color.linear-rgb(255, 252, 225),
  )
  set text(font: "EB Garamond 08", size: 16pt, lang: "de")
  set heading(numbering: "1.1.1", supplement: metadata("Shown"))
  line(start: (0%, 0%), end: (8in, 0%), stroke: (thickness: 2pt))

  align(horizon + left)[
    #text(size: 32pt, title)\
    #v(1em)
    #subtitle
  ]

  align(bottom + left)[
    #link("mailto: marius.niveri@studium.uni-hamburg.de")[Marius Niveri]\
    #datetime.today().display()
    #align(
      right,
      link(faculty)[Universit√§t Hamburg],
    )
  ]
  pagebreak()
  set text(font: "EB Garamond 08", size: 20pt, lang: "de")
  set page(
    margin: auto,
    footer: context [
      #text(
        size: 9pt,
        grid(
          columns: (1fr, 1fr, 1fr),
          align(
            left,
            [
              #title \
              #link("mailto: marius.niveri@studium.uni-hamburg.de")[Marius Niveri]
            ],
          ),
          align(
            center,
            counter(page).display("1 von 1", both: true),
          ),
          align(
            right,
            link(faculty)[Universit√§t Hamburg],
          ),
        ),
      )
    ],
  )
  align(center, text(outline(indent: auto, title: none), size: 15pt))
  pagebreak()
  body
  pagebreak()
  set text(font: "EB Garamond 08", size: 16pt, lang: "de")
  heading(
    "Literaturverzeichnis",
    numbering: none,
    level: 1,
    supplement: metadata("Hidden"),
  )
  bibliography("bib.yaml", title: none)
  v(1em)
  text("Der Quellcode dieser Pr√§sentation ist auf GitHub verf√ºgbar: ")
  link("https://github.com/m4r1vs/presentation-ai-hardware")[github.com/m4r1vs/presentation-ai-hardware]
}

#show heading.where(level: 1, supplement: metadata("Shown")): it => {
  block(width: 100%)[
    #set align(horizon + center)
    #set text(30pt, weight: "bold")
    #v(32%)
    #text(it.body)
  ]
  pagebreak()
}

#show: body => title-page(
  title: [Hardware-Beschleunigung f√ºr ML/AI: GPUs und TPUs],
  subtitle: [
    Seminar Supercomputer: Forschung und Innovation\
    Bei
    #link("mailto: anna.fuchs@uni-hamburg.de")[Anna Fuchs]
    und
    #link("mailto: jannek.squar@uni-hamburg.de")[Jannek Squar]\
  ],
  "https://wr.informatik.uni-hamburg.de/start",
  body,
)


#show figure: it => {
  it.body
  block[
    #set text(size: 12pt) // Adjust the size to be smaller
    #text(it.caption)
  ]
}

#show cite: it => {
  super(it)
}

= Einf√ºhrung

== Aktueller Stand
#v(16pt)

- Markt f√ºr KI-Chips aktuell stark von hochparallelisierbaren\
  Prozessoren dominiert: @NvTheChipBBC
  - GPUs, TPUs, NPUs, etc.
- Microsoft kaufte 2024 eine halbe Millionen NVIDIA Karten (H100, H200, ..) @MicrAcq500Mio
  - Insgesamt √ºber $30$ Milliarden Euro an Kosten.
#v(3em)
#align(
  right,
  [
    #sym.arrow.r.double Warum sind xPUs so gut geeignet f√ºr KI- und ML- Anwendungen?
  ],
)

#pagebreak()

= Berechnung von KI/ML

== Feedforward Neural Network <ff-nn>

- Gewichte werden im Training angepasst.
- Beispiel: $(P_n, D_n)$ Bilder $P$ und deren Beschreibungen $D$ als $n$ Trainingsdaten.
  - $b$ Batches: $n/b$ gro√üe Partitionen der Daten
  - $e$ Epochen: R√ºckpropagation wird $e$-Mal auf den Daten ausgef√ºhrt.

  #sym.arrow.r.double Berechnungen der Batches kann parallel ausgef√ºhrt werden. \
  #sym.arrow.r.double Nach jeder Epoche werden Gewichte aggregiert.

- Alternativ falls Modell zu gro√ü f√ºr Speicher:

  #sym.arrow.r.double Aufteilung der Ebenen auf Prozessoren.

#v(1em)
#align(
  right,
  [
    #sym.arrow.r.double Berechnungen fast ausschlie√ülich Vektor-Matrix Multiplikationen!
  ],
)

#pagebreak()

== Recurrent Neural Networks

- Schlecht zu parallelisieren, da Gradienten voneinander abh√§ngig sind:
#columns(
  2,
  [
    #figure(
      image("Hopfield-net-vector.svg", height: 65%),
      caption: [RNN mit 4 Neuronen @HopRNNSVG],
    )
    #block(fill: color.linear-rgb(236, 226, 160), inset: 8pt, radius: 4pt)[
      Allerdings k√∂nnte sich das bald √§ndern! @ParallelizingNonLinearSeqModels
    ]
  ],
)

#pagebreak()

== Transformer und Generatoren

- 2017 wurde "Attention Is All You Need" von Google ver√∂ffentlicht.
- Grundstein f√ºr *parallelisierbare* Transformer wurde dadurch gelegt. @AttnIsAllYouNeed \
  - Jahr darauf Ver√∂ffentlichung von GPT-1 (Generative Pretrained Transformer 1). @GPT1GitHub

#pagebreak()

=== Simplifizierte Funktionsweise

#align(center)[_"Ich esse den Hamburger"_]

1. Eingabe in Vektoren ("Tokens") aufteilen.
  - z.B. _"Ich"_ $= (231, 231, 534, 4, 321, 342, ...)$
2. Vektorwerte durch $mono("Attention")$ aktualisieren.
  - Sind wir oder #text("üçî", size: 14pt) mit _"Hamburger"_ gemeint?
3. Kontext von Tokens durch Feed-Forward#super(ref(<ff-nn>)) erweitern.
  - z.B. Fakten speichern. @FactFind
4. Schritt 2 und 3 bis zur gew√ºnschten Genauigkeit wiederholen.
#sym.arrow.r.double *Ergebnis* (mit viel mehr Trainingsdaten): \
- Inhaltlich √§hnliche Tokens liegen r√§umlich dicht beieinander.
- Statistisch bestimmbar, welches n√§chste Token am wahrscheinlichsten ist.

#pagebreak()

=== "Aufmerksamkeit" Berechnen
- Beziehungen zwischen W√∂rtern wird durch $mono("Attention")$ berechnet:

#let unimp(x) = text(fill: color.linear-rgb(0, 0, 0, 116), $#x$)

$
  mono("Attention")(Q,K,V) := mono("softmax")((Q K^unimp(T)) / unimp(sqrt(d_k)))V
$

// Queries: Fragen (z.B. Adjektive vor mir?)
// Keys: Antworten (z.B. Ja, hier!)
// Values: Werte   (z.B. √Ñnderungen in der "Farb" Dimension)
#h(1em)Matrizen $Q, K, V$: Queries, Keys, Values \ \
- Gibt an, wie relevant ein Token f√ºr die Aktualisierung eines anderen Tokens ist.
- Kann sehr gut parallelisiert werden ($mono("MultiheadAttention")$).
#v(2em)
#align(
  right,
  [
    #sym.arrow.r.double Berechnungen fast ausschlie√ülich Vektor-Matrix Multiplikationen!
  ],
)

#pagebreak()

= Matrix Multiplikationen Parallel Ausf√ºhren

== Kernel Definieren

*Erinnerung*: Gegeben $arrow(v) = mat(v_1;v_2)$ und $M = mat(M_(1,1), M_(1,2); M_(2,1), M_(2,2))$,
gilt: $arrow(v) dot M = mat(v_1 dot M_(1,1) +v_2 dot M_(2,1);v_1 dot M_(1,2) + v_2 dot M_(2,2))$

#show raw.where(block: true): block.with(
  fill: color.linear-rgb(235, 204, 205),
  inset: 8pt,
  radius: 4pt,
)

#align(center)[
  #text(size: 18pt)[
    ```C
    __global__ void vectorMatrixMulKernel(float *V, float *M, float *RESULT, int n) {
        int row = blockIdx.x * blockDim.x + threadIdx.x;

        if (row < n) {
            float value = 0.0f;
            for (int k = 0; k < n; k++) {
                value += V[k] * M[k * n + row];
            }
            RESULT[row] = value;
        }
    }
    ```]
]

#pagebreak()

== Kernel Ausf√ºhren

#align(center)[
  #text(size: 16pt)[
    ```C
    void vecMatMul(float *V, float *M, float *RESULT, int n) {
      float *d_V, *d_M, *d_RESULT;

      cudaMalloc(&d_V, n * sizeof(float));
      // .. gleiches f√ºr M und RESULT

      cudaMemcpy(d_V, V, n * sizeof(float), cudaMemcpyHostToDevice);
      // .. gleiches f√ºr M

      int blockSize = 256; // Je nach GPU Modell und Matrix Gr√∂√üe
      int gridSize = (n + blockSize - 1) / blockSize;

      vectorMatrixMulKernel<<<gridSize, blockSize>>>(d_V, d_M, d_RESULT, n);

      cudaMemcpy(C, d_C, sizeVector, cudaMemcpyDeviceToHost);
      // TODO: Speicher freigeben mit cudaFree();
    };
    ```]
]

#pagebreak()

== Was passiert in der Hardware?

#figure(
  image("./cpu-gpu-mem.png", height: 80%),
  caption: [CPU und GPU Abbildung],
)

#pagebreak()

= Zusammenfassung

== Vergleich von "Matrix Proccessing Units"

=== TPU
- TPU #sym.arrow.l.r.double "Tensor Processing Unit"
  - Stark von Google gepr√§gter Begriff.
  - Leicht energieeffizienter als GPUs der gleichen Generation.@TPUvsGPU

=== NPU
- NPU #sym.arrow.l.r.double "Neural Processing Unit"
  - Nicht klar definierter Begriff.
  - Meist werden Chips in mobilen Endger√§ten bezeichnet, die ML-Modelle ausf√ºhren sollen.
    - z.B. "Neural Engine" von Apple f√ºr "Apple Intelligence".@NeuralEngineAAPL
  - Manchmal auch $text("TPU"), text("GPU") in text("NPU")$

#pagebreak()

== Brauchen wir GPUs im HPC?
#v(2em)
*Ja*, weil... \
#h(1em) ... GPUs sind sehr gut geeignet f√ºr maschinelles Lernen und KI. Ein Trend der bleibt. \
#h(1em) ... Durch gekonnte Programmierung k√∂nnen auch viele andere Programme \
#h(2em) durch GPUs beschleunigt werden.

#pagebreak()

== Neue NVIDIA H200

#columns(
  2,
  [
    #figure(
      image("./llm-inference-chart.svg", height: 65%),
      caption: [NVIDIA H200 Promo Material @NvdaPromoH200],
    )
    #figure(
      image("./energy-tco-chart.svg", height: 80%),
      caption: [NVIDIA H200 Promo Material @NvdaPromoH200],
    )
  ],
)

#figure(
  image("./cache_misses.png", height: 80%),
  caption: [Schlechte Lokalit√§t vom Speicher @CacheLocalGPU],
)
#figure(
  image("./good_cache_locality.png", height: 80%),
  caption: [Gute Speicherlokalit√§t @CacheLocalGPU],
)
